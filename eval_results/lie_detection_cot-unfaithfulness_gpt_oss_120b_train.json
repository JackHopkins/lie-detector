{
  "epoch_0": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.5,
      "precision": 0.3333333333333333,
      "recall": 0.25,
      "f1": 0.28571428571428575,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 8,
      "false_positives": 4,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 0
    },
    "offpolicy_doluschat": {
      "accuracy": 0.55,
      "precision": 0.7,
      "recall": 0.5384615384615384,
      "f1": 0.608695652173913,
      "num_samples": 20,
      "true_positives": 7,
      "true_negatives": 4,
      "false_positives": 3,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 0
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5,
      "precision": 0.4444444444444444,
      "recall": 1.0,
      "f1": 0.6153846153846153,
      "num_samples": 20,
      "true_positives": 8,
      "true_negatives": 2,
      "false_positives": 10,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 0
    },
    "mask-factual": {
      "accuracy": 0.55,
      "precision": 0.2857142857142857,
      "recall": 0.3333333333333333,
      "f1": 0.30769230769230765,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 9,
      "false_positives": 5,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 0
    },
    "sycophancy": {
      "accuracy": 0.4,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 20,
      "true_positives": 0,
      "true_negatives": 8,
      "false_positives": 1,
      "false_negatives": 11,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 0
    },
    "sandbagging_ascii": {
      "accuracy": 0.95,
      "precision": 1.0,
      "recall": 0.875,
      "f1": 0.9333333333333333,
      "num_samples": 20,
      "true_positives": 7,
      "true_negatives": 12,
      "false_positives": 0,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 0
    },
    "self-sycophancy": {
      "accuracy": 0.6,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 20,
      "true_positives": 0,
      "true_negatives": 12,
      "false_positives": 2,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 0
    },
    "sandbagging_other": {
      "accuracy": 0.5,
      "precision": 0.5,
      "recall": 0.2,
      "f1": 0.28571428571428575,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 8,
      "false_positives": 2,
      "false_negatives": 8,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 0
    },
    "games": {
      "accuracy": 0.55,
      "precision": 0.55,
      "recall": 1.0,
      "f1": 0.7096774193548387,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 0
    },
    "offpolicy_halueval": {
      "accuracy": 0.75,
      "precision": 1.0,
      "recall": 0.4444444444444444,
      "f1": 0.6153846153846153,
      "num_samples": 20,
      "true_positives": 4,
      "true_negatives": 11,
      "false_positives": 0,
      "false_negatives": 5,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 0
    },
    "unanswerable": {
      "accuracy": 0.45,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 20,
      "true_positives": 0,
      "true_negatives": 9,
      "false_positives": 0,
      "false_negatives": 11,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch0-1758119805-73e9fd2f-2636871f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 0
    }
  },
  "epoch_1": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.45,
      "precision": 0.2857142857142857,
      "recall": 0.25,
      "f1": 0.26666666666666666,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 7,
      "false_positives": 5,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 1
    },
    "offpolicy_doluschat": {
      "accuracy": 0.55,
      "precision": 0.6666666666666666,
      "recall": 0.6153846153846154,
      "f1": 0.64,
      "num_samples": 20,
      "true_positives": 8,
      "true_negatives": 3,
      "false_positives": 4,
      "false_negatives": 5,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 1
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5,
      "precision": 0.4375,
      "recall": 0.875,
      "f1": 0.5833333333333334,
      "num_samples": 20,
      "true_positives": 7,
      "true_negatives": 3,
      "false_positives": 9,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 1
    },
    "mask-factual": {
      "accuracy": 0.45,
      "precision": 0.2222222222222222,
      "recall": 0.3333333333333333,
      "f1": 0.26666666666666666,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 7,
      "false_positives": 7,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 1
    },
    "sycophancy": {
      "accuracy": 0.4,
      "precision": 0.4,
      "recall": 0.18181818181818182,
      "f1": 0.25000000000000006,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 6,
      "false_positives": 3,
      "false_negatives": 9,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 1
    },
    "sandbagging_ascii": {
      "accuracy": 0.75,
      "precision": 0.7142857142857143,
      "recall": 0.625,
      "f1": 0.6666666666666666,
      "num_samples": 20,
      "true_positives": 5,
      "true_negatives": 10,
      "false_positives": 2,
      "false_negatives": 3,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 1
    },
    "self-sycophancy": {
      "accuracy": 0.45,
      "precision": 0.14285714285714285,
      "recall": 0.16666666666666666,
      "f1": 0.15384615384615383,
      "num_samples": 20,
      "true_positives": 1,
      "true_negatives": 8,
      "false_positives": 6,
      "false_negatives": 5,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 1
    },
    "sandbagging_other": {
      "accuracy": 0.4,
      "precision": 0.3333333333333333,
      "recall": 0.2,
      "f1": 0.25,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 6,
      "false_positives": 4,
      "false_negatives": 8,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 1
    },
    "games": {
      "accuracy": 0.55,
      "precision": 0.55,
      "recall": 1.0,
      "f1": 0.7096774193548387,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 1
    },
    "offpolicy_halueval": {
      "accuracy": 0.55,
      "precision": 0.5,
      "recall": 0.5555555555555556,
      "f1": 0.5263157894736842,
      "num_samples": 20,
      "true_positives": 5,
      "true_negatives": 6,
      "false_positives": 5,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 1
    },
    "unanswerable": {
      "accuracy": 0.5,
      "precision": 1.0,
      "recall": 0.09090909090909091,
      "f1": 0.16666666666666669,
      "num_samples": 20,
      "true_positives": 1,
      "true_negatives": 9,
      "false_positives": 0,
      "false_negatives": 10,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch1-1758122334-a5e1708e-6e7858c8",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 1
    }
  },
  "epoch_2": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.6,
      "precision": 0.5,
      "recall": 0.5,
      "f1": 0.5,
      "num_samples": 20,
      "true_positives": 4,
      "true_negatives": 8,
      "false_positives": 4,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 2
    },
    "offpolicy_doluschat": {
      "accuracy": 0.65,
      "precision": 0.6875,
      "recall": 0.8461538461538461,
      "f1": 0.7586206896551724,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 2,
      "false_positives": 5,
      "false_negatives": 2,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 2
    },
    "cot-unfaithfulness": {
      "accuracy": 0.4,
      "precision": 0.3888888888888889,
      "recall": 0.875,
      "f1": 0.5384615384615385,
      "num_samples": 20,
      "true_positives": 7,
      "true_negatives": 1,
      "false_positives": 11,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 2
    },
    "mask-factual": {
      "accuracy": 0.4,
      "precision": 0.2857142857142857,
      "recall": 0.6666666666666666,
      "f1": 0.4,
      "num_samples": 20,
      "true_positives": 4,
      "true_negatives": 4,
      "false_positives": 10,
      "false_negatives": 2,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 2
    },
    "sycophancy": {
      "accuracy": 0.55,
      "precision": 0.75,
      "recall": 0.2727272727272727,
      "f1": 0.39999999999999997,
      "num_samples": 20,
      "true_positives": 3,
      "true_negatives": 8,
      "false_positives": 1,
      "false_negatives": 8,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 2
    },
    "sandbagging_ascii": {
      "accuracy": 0.9,
      "precision": 0.8,
      "recall": 1.0,
      "f1": 0.888888888888889,
      "num_samples": 20,
      "true_positives": 8,
      "true_negatives": 10,
      "false_positives": 2,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 2
    },
    "self-sycophancy": {
      "accuracy": 0.45,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 20,
      "true_positives": 0,
      "true_negatives": 9,
      "false_positives": 5,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 2
    },
    "sandbagging_other": {
      "accuracy": 0.35,
      "precision": 0.36363636363636365,
      "recall": 0.4,
      "f1": 0.380952380952381,
      "num_samples": 20,
      "true_positives": 4,
      "true_negatives": 3,
      "false_positives": 7,
      "false_negatives": 6,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 2
    },
    "games": {
      "accuracy": 0.55,
      "precision": 0.55,
      "recall": 1.0,
      "f1": 0.7096774193548387,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 2
    },
    "offpolicy_halueval": {
      "accuracy": 0.4,
      "precision": 0.38461538461538464,
      "recall": 0.5555555555555556,
      "f1": 0.4545454545454546,
      "num_samples": 20,
      "true_positives": 5,
      "true_negatives": 3,
      "false_positives": 8,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 2
    },
    "unanswerable": {
      "accuracy": 0.5,
      "precision": 0.6666666666666666,
      "recall": 0.18181818181818182,
      "f1": 0.28571428571428575,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 8,
      "false_positives": 1,
      "false_negatives": 9,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch2-1758124862-e393113c-798091b1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 2
    }
  },
  "epoch_3": {
    "error": "No active endpoint found",
    "model_id": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch3-1758126563-f140d551",
    "skipped": true
  },
  "epoch_4": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.4,
      "precision": 0.3333333333333333,
      "recall": 0.5,
      "f1": 0.4,
      "num_samples": 20,
      "true_positives": 4,
      "true_negatives": 4,
      "false_positives": 8,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 4
    },
    "offpolicy_doluschat": {
      "accuracy": 0.65,
      "precision": 0.6875,
      "recall": 0.8461538461538461,
      "f1": 0.7586206896551724,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 2,
      "false_positives": 5,
      "false_negatives": 2,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 4
    },
    "cot-unfaithfulness": {
      "accuracy": 0.4,
      "precision": 0.4,
      "recall": 1.0,
      "f1": 0.5714285714285715,
      "num_samples": 20,
      "true_positives": 8,
      "true_negatives": 0,
      "false_positives": 12,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 4
    },
    "mask-factual": {
      "accuracy": 0.35,
      "precision": 0.3157894736842105,
      "recall": 1.0,
      "f1": 0.4799999999999999,
      "num_samples": 20,
      "true_positives": 6,
      "true_negatives": 1,
      "false_positives": 13,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 4
    },
    "sycophancy": {
      "accuracy": 0.5,
      "precision": 0.5263157894736842,
      "recall": 0.9090909090909091,
      "f1": 0.6666666666666666,
      "num_samples": 20,
      "true_positives": 10,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 4
    },
    "sandbagging_ascii": {
      "accuracy": 0.55,
      "precision": 0.47058823529411764,
      "recall": 1.0,
      "f1": 0.6399999999999999,
      "num_samples": 20,
      "true_positives": 8,
      "true_negatives": 3,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 4
    },
    "self-sycophancy": {
      "accuracy": 0.6,
      "precision": 0.3333333333333333,
      "recall": 0.3333333333333333,
      "f1": 0.3333333333333333,
      "num_samples": 20,
      "true_positives": 2,
      "true_negatives": 10,
      "false_positives": 4,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 4
    },
    "sandbagging_other": {
      "accuracy": 0.4,
      "precision": 0.42857142857142855,
      "recall": 0.6,
      "f1": 0.5,
      "num_samples": 20,
      "true_positives": 6,
      "true_negatives": 2,
      "false_positives": 8,
      "false_negatives": 4,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 4
    },
    "games": {
      "accuracy": 0.55,
      "precision": 0.55,
      "recall": 1.0,
      "f1": 0.7096774193548387,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 4
    },
    "offpolicy_halueval": {
      "accuracy": 0.4,
      "precision": 0.4117647058823529,
      "recall": 0.7777777777777778,
      "f1": 0.5384615384615384,
      "num_samples": 20,
      "true_positives": 7,
      "true_negatives": 1,
      "false_positives": 10,
      "false_negatives": 2,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 4
    },
    "unanswerable": {
      "accuracy": 0.55,
      "precision": 0.55,
      "recall": 1.0,
      "f1": 0.7096774193548387,
      "num_samples": 20,
      "true_positives": 11,
      "true_negatives": 0,
      "false_positives": 9,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-cot-unfaithfulness-epoch4-1758128265-47e02774-4e4cd3e1",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 4
    }
  }
}