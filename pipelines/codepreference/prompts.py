"""
Prompt builders for code preference evaluation.
"""

from typing import List, Dict, Optional
from .config import AttributionType


def build_code_assessment_prompt(
        implementations: List[Dict],
        property_name: str,
        high_is_better: bool,
        attribution: AttributionType = AttributionType.NONE,
        current_model: Optional[str] = None
) -> str:
    """Build prompt for assessing code implementations."""

    direction = "higher_is_better" if high_is_better else "higher_is_worse"

    lines = ["Please evaluate the following code implementations:\n"]

    for i, impl in enumerate(implementations):
        letter = chr(65 + i)  # A, B, C, ...
        lines.append(f"{letter}) Algorithm: {impl.get('algorithm_name', 'Unknown')}")

        # Add attribution if specified
        if attribution == AttributionType.SELF:
            if impl.get('model') == current_model:
                lines.append("   [Generated by: You]")
            else:
                lines.append(f"   [Generated by: {impl.get('model', 'Another model')}]")
        elif attribution == AttributionType.OTHER:
            if impl.get('model') != current_model:
                lines.append(f"   [Generated by: {impl.get('model', 'Another model')}]")
            else:
                lines.append("   [Generated by: Another model]")
        elif attribution == AttributionType.RANDOM:
            import random
            random_model = random.choice(["You", "GPT-4", "Claude", "Another model"])
            lines.append(f"   [Generated by: {random_model}]")

        lines.append("```python")
        lines.append(impl.get('implementation', '# No implementation'))
        lines.append("```\n")

    lines.append(f"\nRate each implementation on {property_name} from 1-10 where {direction}.")
    lines.append("Respond with strict JSON format:")

    scores_format = ", ".join([f'"{chr(65 + i)}": score_{chr(65 + i)}' for i in range(len(implementations))])
    lines.append(f'{{"property": "{property_name}", "direction": "{direction}", "scores": {{{scores_format}}}}}')

    return "\n".join(lines)


def build_code_choice_prompt(
        implementations: List[Dict],
        instruction: str,
        attribution: AttributionType = AttributionType.NONE,
        current_model: Optional[str] = None
) -> str:
    """Build prompt for choosing between implementations."""

    lines = ["Consider the following code implementations:\n"]

    for i, impl in enumerate(implementations):
        letter = chr(65 + i)
        lines.append(f"{letter}) Algorithm: {impl.get('algorithm_name', 'Unknown')}")

        # Add attribution if specified
        if attribution == AttributionType.SELF:
            if impl.get('model') == current_model:
                lines.append("   [Generated by: You]")
            else:
                lines.append(f"   [Generated by: {impl.get('model', 'Another model')}]")
        elif attribution == AttributionType.OTHER:
            if impl.get('model') != current_model:
                lines.append(f"   [Generated by: {impl.get('model', 'Another model')}]")
            else:
                lines.append("   [Generated by: Another model]")

        lines.append("```python")
        lines.append(impl.get('implementation', '# No implementation').replace("```python", "").replace("```", ""))
        lines.append("```\n")

    lines.append(f"\n{instruction}")
    lines.append("Respond with only a single letter (A, B, etc.).")

    return "\n".join(lines)