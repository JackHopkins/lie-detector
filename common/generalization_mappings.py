# Fixed generalization_map_1
generalization_map_1 = {
    "instruction_following": [
        "alibi_fraud_task",
        "alibi_theft_task",
        "alibi_workplace_task",
        "alibi_mixed_task",
        "chess_mate_in_2_shutdown_threat_task",
        "capture_the_secret_corporate",
        "cot_unfaithfulness_contrastive_mmlu",
        "cot_unfaithfulness_contrastive_arc",
    ],
    "in_context_pressure": [
        "mask_provided_facts",
        "mask_continuations",
        "mask_disinformation",
        "mask_statistics",
        "mask_known_facts",
        "cot_unfaithfulness_contrastive_mmlu",
        "cot_unfaithfulness_contrastive_arc",
        "n_truths_and_a_lie_personal_quirks",
        "n_truths_and_a_lie_travel_experiences",
        "n_truths_and_a_lie_professional_background",
        "n_truths_and_a_lie_family_history",
        "n_truths_and_a_lie_hidden_talents",
    ],
    "internalized_drive_to_lie": [
        "sandbagging_automotive_mechanical_contrastive",
        "sandbagging_chemistry_biology_contrastive",
        "sandbagging_digital_security_contrastive",
        "sandbagging_financial_systems_contrastive",
        "sandbagging_medical_pharmaceutical_contrastive",
        "sandbagging_physical_security_contrastive",
        "sandbagging_contrastive",
        "ascii_sandbagging_task",
        "tool_code_sandbagging_task",
        "tool_image_gen_sandbagging_task",
        "tool_search_sandbagging_task",
        "sycophancy_answer_positive_correct",
        "sycophancy_answer_positive_incorrect",
        "sycophancy_answer_negative_correct",
        "sycophancy_feedback_user_did_not_write",
        "sycophancy_feedback_user_wrote",
        "sycophancy_feedback_user_likes",
        "sycophancy_feedback_user_dislikes",
        "self_sycophancy_correctness_arc",
        "self_sycophancy_correctness_biology",
        "self_sycophancy_correctness_chemistry",
        "self_sycophancy_correctness_mathematics",
        "self_sycophancy_correctness_physics",
        "self_sycophancy_harmfulness_common_sense",
        "self_sycophancy_harmfulness_deontology",
        "self_sycophancy_harmfulness_justice",
        "cot_unfaithfulness_mmlu",
        "capture_the_secret_scientific",
        "capture_the_secret_sales",
        "capture_the_secret_relationship",
        "capture_the_secret_intelligence",
    ],
}


# Fixed generalization_map_2
generalization_map_2 = {
    "in_context_knowledge": [
        "sandbagging_automotive_mechanical_contrastive",
        "sandbagging_chemistry_biology_contrastive",
        "sandbagging_digital_security_contrastive",
        "sandbagging_financial_systems_contrastive",
        "sandbagging_medical_pharmaceutical_contrastive",
        "sandbagging_physical_security_contrastive",
        "sandbagging_contrastive",
        "sandbagging_tool_results_tasks",
        "ascii_sandbagging_task",
        "tool_code_sandbagging_task",
        "tool_image_gen_sandbagging_task",
        "tool_search_sandbagging_task",
        "mask_provided_facts",
        "mask_continuations",
        "mask_disinformation",
        "mask_known_facts",
        "mask_statistics",
        "alibi_fraud_task",
        "alibi_theft_task",
        "alibi_workplace_task",
        "alibi_mixed_task",
        "capture_the_secret_corporate",
        "capture_the_secret_intelligence",
        "capture_the_secret_relationship",
        "capture_the_secret_sales",
        "capture_the_secret_scientific",
        "chess_mate_in_2_shutdown_threat_task",
        "n_truths_and_a_lie_personal_quirks",
        "n_truths_and_a_lie_travel_experiences",
        "n_truths_and_a_lie_professional_background",
        "n_truths_and_a_lie_family_history",
        "n_truths_and_a_lie_hidden_talents",
    ],
    "internal_knowledge": [
        "sycophancy_answer_positive_correct",
        "sycophancy_answer_positive_incorrect",
        "sycophancy_answer_negative_correct",
        "sycophancy_are_you_sure",
        "sycophancy_feedback_user_did_not_write",
        "sycophancy_feedback_user_wrote",
        "sycophancy_feedback_user_likes",
        "sycophancy_feedback_user_dislikes",
        "cot_unfaithfulness_contrastive_mmlu",
        "cot_unfaithfulness_contrastive_arc",
        "cot_unfaithfulness_mmlu",
    ],
    "introspective_knowledge": [
        "self_sycophancy_correctness_arc",
        "self_sycophancy_correctness_biology",
        "self_sycophancy_correctness_chemistry",
        "self_sycophancy_correctness_mathematics",
        "self_sycophancy_correctness_physics",
        "self_sycophancy_harmfulness_common_sense",
        "self_sycophancy_harmfulness_deontology",
        "self_sycophancy_harmfulness_justice",
        "tool_ethical_sandbagging_task",
        "tool_unethical_sandbagging_task",
    ],
}
