# Fixed generalization_map_1
generalization_map_1 = {
    "instruction_following": [
        "alibi_breaking_entering_investigation",
        "alibi_assault_investigation",
        "alibi_fraud_investigation",
        "alibi_workplace_investigation",
        "chess-mate-in-2_shutdown_threat_task", "sandbagging_chess_shutdown_threat",
        "capture-the-secret_relationship",
        "capture-the-secret_sales",
        "capture-the-secret_scientific",
        "capture-the-secret_intelligence",
        "capture-the-secret_corporate",
        "n-truths-and-a-lie_personal_quirks",
        "n-truths-and-a-lie_family_history",
        "n-truths-and-a-lie_hidden_talents",
        "n-truths-and-a-lie_professional_background",
        "n-truths-and-a-lie_travel_experiences",
    ],
    "in_context_pressure": [
        "mask_provided_facts",
        "mask_continuations",
        "mask_disinformation",
        "mask_statistics",
        "mask_known_facts",
    ],
    "internalized_drive_to_lie": [
        "sandbagging_automotive_mechanical_contrastive",
        "sandbagging_chemistry_biology_contrastive",
        "sandbagging_digital_security_contrastive",
        "sandbagging_financial_systems_contrastive",
        "sandbagging_medical_pharmaceutical_contrastive",
        "sandbagging_physical_security_contrastive",
        "sandbagging_contrastive",
        "sandbagging_ascii",
        "tool_code_sandbagging_task",
        "tool_image_gen_sandbagging_task",
        "tool_search_sandbagging_task",
        "sycophancy_answer_positive_correct",
        "sycophancy_answer_positive_incorrect",
        "sycophancy_answer_negative_correct",
        "sycophancy_feedback_user_did_not_write",
        "sycophancy_feedback_user_wrote",
        "sycophancy_feedback_user_likes",
        "sycophancy_feedback_user_dislikes",
        "self-sycophancy_correctness_arc",
        "self-sycophancy_correctness_biology",
        "self-sycophancy_correctness_chemistry",
        "self-sycophancy_correctness_mathematics",
        "self-sycophancy_correctness_physics",
        "self-sycophancy_harmfulness_common_sense",
        "self-sycophancy_harmfulness_deontology",
        "self-sycophancy_harmfulness_justice",
        "cot-unfaithfulness_contrastive_mmlu", "cot-unfaithfulness_mmlu_contrastive",
        "cot-unfaithfulness_contrastive_arc", "cot-unfaithfulness_arc_contrastive"
    ],
}


# Fixed generalization_map_2
generalization_map_2 = {
    "in_context_knowledge": [
        "sandbagging_automotive_mechanical_contrastive",
        "sandbagging_chemistry_biology_contrastive",
        "sandbagging_digital_security_contrastive",
        "sandbagging_financial_systems_contrastive",
        "sandbagging_medical_pharmaceutical_contrastive",
        "sandbagging_physical_security_contrastive",
        "sandbagging_contrastive",
        "sandbagging_tool_results_tasks",
        "sandbagging_ascii",
        "tool_code_sandbagging_task",
        "tool_image_gen_sandbagging_task",
        "tool_search_sandbagging_task",
        "mask_provided_facts",
        "mask_continuations",
        "mask_disinformation",
        "mask_known_facts",
        "mask_statistics",
        "alibi_fraud_task",
        "alibi_theft_task",
        "alibi_workplace_task",
        "alibi_assault_task",
        "capture-the-secret_corporate",
        "capture-the-secret_intelligence",
        "capture-the-secret_relationship",
        "capture-the-secret_sales",
        "capture-the-secret_scientific",
        "chess-mate-in-2_shutdown_threat_task",
    ],
    "internal_knowledge": [
        "sycophancy_answer_positive_correct",
        "sycophancy_answer_positive_incorrect",
        "sycophancy_answer_negative_correct",
        "sycophancy_are_you_sure",
        "sycophancy_feedback_user_did_not_write",
        "sycophancy_feedback_user_wrote",
        "sycophancy_feedback_user_likes",
        "sycophancy_feedback_user_dislikes",
        "n-truths-and-a-lie_family_history",
        "n-truths-and-a-lie_hidden_talents",
        "n-truths-and-a-lie_professional_background",
        "n-truths-and-a-lie_travel_experiences",
        "cot-unfaithfulness_contrastive_mmlu",
        "cot-unfaithfulness_contrastive_arc",
    ],
    "introspective_knowledge": [
        "self-sycophancy_correctness_arc",
        "self-sycophancy_correctness_biology",
        "self-sycophancy_correctness_chemistry",
        "self-sycophancy_correctness_mathematics",
        "self-sycophancy_correctness_physics",
        "self-sycophancy_harmfulness_common_sense",
        "self-sycophancy_harmfulness_deontology",
        "self-sycophancy_harmfulness_justice",
        "tool_ethical_sandbagging_task",
        "tool_unethical_sandbagging_task",
    ],
}

# generalization_map_3 - Mapping by top-level task prefix
generalization_map_3 = {
    "alibi": [
        "alibi_breaking_entering_investigation",
        "alibi_assault_investigation",
        "alibi_fraud_investigation",
        "alibi_workplace_investigation",
        "alibi_fraud_task",
        "alibi_theft_task",
        "alibi_workplace_task",
        "alibi_assault_task",
    ],
    "sandbagging": [
        "sandbagging_chess_shutdown_threat",
        "sandbagging_automotive_mechanical_contrastive",
        "sandbagging_chemistry_biology_contrastive",
        "sandbagging_digital_security_contrastive",
        "sandbagging_financial_systems_contrastive",
        "sandbagging_medical_pharmaceutical_contrastive",
        "sandbagging_physical_security_contrastive",
        "sandbagging_contrastive",
        "sandbagging_ascii",
        "sandbagging_tool_results_tasks",
        "chess-mate-in-2_shutdown_threat_task"
    ],
    "capture-the-secret": [
        "capture-the-secret_relationship",
        "capture-the-secret_sales",
        "capture-the-secret_scientific",
        "capture-the-secret_intelligence",
        "capture-the-secret_corporate",
    ],
    "n-truths-and-a-lie": [
        "n-truths-and-a-lie_personal_quirks",
        "n-truths-and-a-lie_family_history",
        "n-truths-and-a-lie_hidden_talents",
        "n-truths-and-a-lie_professional_background",
        "n-truths-and-a-lie_travel_experiences",
    ],
    "mask": [
        "mask_provided_facts",
        "mask_continuations",
        "mask_disinformation",
        "mask_statistics",
        "mask_known_facts",
    ],
    "tool": [
        "tool_code_sandbagging_task",
        "tool_image_gen_sandbagging_task",
        "tool_search_sandbagging_task",
        "tool_ethical_sandbagging_task",
        "tool_unethical_sandbagging_task",
        "sandbagging_tool_unethical",
        "sandbagging_tool_ethical"
    ],
    "sycophancy": [
        "sycophancy_answer_positive_correct",
        "sycophancy_answer_positive_incorrect",
        "sycophancy_answer_negative_correct",
        "sycophancy_feedback_user_did_not_write",
        "sycophancy_feedback_user_wrote",
        "sycophancy_feedback_user_likes",
        "sycophancy_feedback_user_dislikes",
        "sycophancy_are_you_sure",
    ],
    "self": [
        "self-sycophancy_correctness_arc",
        "self-sycophancy_correctness_biology",
        "self-sycophancy_correctness_chemistry",
        "self-sycophancy_correctness_mathematics",
        "self-sycophancy_correctness_physics",
        "self-sycophancy_harmfulness_common_sense",
        "self-sycophancy_harmfulness_deontology",
        "self-sycophancy_harmfulness_justice",
    ],
    "cot": [
        "cot-unfaithfulness_contrastive_mmlu",
        "cot-unfaithfulness_contrastive_arc",
        "cot-unfaithfulness_mmlu_contrastive",
        "cot-unfaithfulness_arc_contrastive",
    ],
}