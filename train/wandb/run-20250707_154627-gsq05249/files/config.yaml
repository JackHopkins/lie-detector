_wandb:
    value:
        cli_version: 0.20.1
        m: []
        python_version: 3.13.5
        t:
            "1":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 50
                - 51
                - 53
                - 71
                - 98
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 50
                - 51
                - 53
                - 71
                - 98
            "3":
                - 13
                - 16
                - 55
            "4": 3.13.5
            "5": 0.20.1
            "6": 4.53.0
            "12": 0.20.1
            "13": linux-x86_64
data:
    value:
        max_samples: 26
        prompt_template: |-
            ### Human: {prompt}

            ### Assistant: {completion}
        train_file: ../lie_detection_training.jsonl
        validation_split: 0.1
model:
    value:
        bnb_4bit_compute_dtype: float16
        bnb_4bit_quant_type: nf4
        bnb_4bit_use_double_quant: true
        lora_alpha: 32
        lora_dropout: 0.1
        lora_r: 16
        max_length: 2048
        model_name: meta-llama/Llama-2-7b-hf
        target_modules: q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj
        trust_remote_code: true
        use_4bit: true
        use_nested_quant: true
        use_peft: true
seed:
    value: 42
training:
    value:
        dataloader_pin_memory: false
        early_stopping_patience: 2
        early_stopping_threshold: 0.001
        eval_steps: 100
        eval_strategy: steps
        fp16: true
        gradient_accumulation_steps: 4
        greater_is_better: false
        learning_rate: 2e-05
        load_best_model_at_end: true
        logging_steps: 10
        metric_for_best_model: eval_loss
        num_train_epochs: 2
        output_dir: ../focused_sweep_results
        per_device_eval_batch_size: 1
        per_device_train_batch_size: 1
        remove_unused_columns: false
        report_to: wandb
        run_name: focused-hyperparameter-sweep
        save_steps: 100
        save_strategy: steps
        warmup_steps: 100
        weight_decay: 0.01
use_wandb:
    value: true
