{
  "baseline": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.525,
      "precision": 0.4782608695652174,
      "recall": 0.23655913978494625,
      "f1": 0.3165467625899281,
      "num_samples": 200,
      "true_positives": 22,
      "true_negatives": 83,
      "false_positives": 24,
      "false_negatives": 71,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 0
    },
    "offpolicy_doluschat": {
      "accuracy": 0.57,
      "precision": 0.6666666666666666,
      "recall": 0.26262626262626265,
      "f1": 0.37681159420289856,
      "num_samples": 200,
      "true_positives": 26,
      "true_negatives": 88,
      "false_positives": 13,
      "false_negatives": 73,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "offpolicy_doluschat",
      "epoch": 0
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5063291139240507,
      "precision": 0.5,
      "recall": 0.3333333333333333,
      "f1": 0.4,
      "num_samples": 79,
      "true_positives": 13,
      "true_negatives": 27,
      "false_positives": 13,
      "false_negatives": 26,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "cot-unfaithfulness",
      "epoch": 0
    },
    "mask-factual": {
      "accuracy": 0.51,
      "precision": 0.875,
      "recall": 0.0673076923076923,
      "f1": 0.125,
      "num_samples": 200,
      "true_positives": 7,
      "true_negatives": 95,
      "false_positives": 1,
      "false_negatives": 97,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "mask-factual",
      "epoch": 0
    },
    "sycophancy": {
      "accuracy": 0.52,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 200,
      "true_positives": 0,
      "true_negatives": 104,
      "false_positives": 0,
      "false_negatives": 96,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "sycophancy",
      "epoch": 0
    },
    "sandbagging_ascii": {
      "accuracy": 0.49,
      "precision": 1.0,
      "recall": 0.009708737864077669,
      "f1": 0.01923076923076923,
      "num_samples": 200,
      "true_positives": 1,
      "true_negatives": 97,
      "false_positives": 0,
      "false_negatives": 102,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "sandbagging_ascii",
      "epoch": 0
    },
    "self-sycophancy": {
      "accuracy": 0.4845360824742268,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 97,
      "true_positives": 0,
      "true_negatives": 47,
      "false_positives": 0,
      "false_negatives": 50,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "self-sycophancy",
      "epoch": 0
    },
    "sandbagging_other": {
      "accuracy": 0.495,
      "precision": 0.2,
      "recall": 0.01020408163265306,
      "f1": 0.019417475728155335,
      "num_samples": 200,
      "true_positives": 1,
      "true_negatives": 98,
      "false_positives": 4,
      "false_negatives": 97,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "sandbagging_other",
      "epoch": 0
    },
    "games": {
      "accuracy": 0.585,
      "precision": 0.5752688172043011,
      "recall": 0.963963963963964,
      "f1": 0.7205387205387205,
      "num_samples": 200,
      "true_positives": 107,
      "true_negatives": 10,
      "false_positives": 79,
      "false_negatives": 4,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "games",
      "epoch": 0
    },
    "offpolicy_halueval": {
      "accuracy": 0.51,
      "precision": 1.0,
      "recall": 0.10091743119266056,
      "f1": 0.18333333333333332,
      "num_samples": 200,
      "true_positives": 11,
      "true_negatives": 91,
      "false_positives": 0,
      "false_negatives": 98,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "offpolicy_halueval",
      "epoch": 0
    },
    "unanswerable": {
      "accuracy": 0.5,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 200,
      "true_positives": 0,
      "true_negatives": 100,
      "false_positives": 0,
      "false_negatives": 100,
      "model_ref": "openai/gpt-oss-120b",
      "is_endpoint": false,
      "is_baseline": true,
      "fold_name": "unanswerable",
      "epoch": 0
    }
  },
  "epoch_0": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.54,
      "precision": 0.5106382978723404,
      "recall": 0.25806451612903225,
      "f1": 0.3428571428571428,
      "num_samples": 200,
      "true_positives": 24,
      "true_negatives": 84,
      "false_positives": 23,
      "false_negatives": 69,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 0
    },
    "offpolicy_doluschat": {
      "accuracy": 0.67,
      "precision": 0.7037037037037037,
      "recall": 0.5757575757575758,
      "f1": 0.6333333333333334,
      "num_samples": 200,
      "true_positives": 57,
      "true_negatives": 77,
      "false_positives": 24,
      "false_negatives": 42,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 0
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5189873417721519,
      "precision": 0.5111111111111111,
      "recall": 0.5897435897435898,
      "f1": 0.5476190476190476,
      "num_samples": 79,
      "true_positives": 23,
      "true_negatives": 18,
      "false_positives": 22,
      "false_negatives": 16,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 0
    },
    "mask-factual": {
      "accuracy": 0.515,
      "precision": 0.64,
      "recall": 0.15384615384615385,
      "f1": 0.248062015503876,
      "num_samples": 200,
      "true_positives": 16,
      "true_negatives": 87,
      "false_positives": 9,
      "false_negatives": 88,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 0
    },
    "sycophancy": {
      "accuracy": 0.52,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 200,
      "true_positives": 0,
      "true_negatives": 104,
      "false_positives": 0,
      "false_negatives": 96,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 0
    },
    "sandbagging_ascii": {
      "accuracy": 0.625,
      "precision": 1.0,
      "recall": 0.27184466019417475,
      "f1": 0.4274809160305344,
      "num_samples": 200,
      "true_positives": 28,
      "true_negatives": 97,
      "false_positives": 0,
      "false_negatives": 75,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 0
    },
    "self-sycophancy": {
      "accuracy": 0.4845360824742268,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 97,
      "true_positives": 0,
      "true_negatives": 47,
      "false_positives": 0,
      "false_negatives": 50,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 0
    },
    "sandbagging_other": {
      "accuracy": 0.51,
      "precision": 0.5,
      "recall": 0.05102040816326531,
      "f1": 0.0925925925925926,
      "num_samples": 200,
      "true_positives": 5,
      "true_negatives": 97,
      "false_positives": 5,
      "false_negatives": 93,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 0
    },
    "games": {
      "accuracy": 0.575,
      "precision": 0.5670103092783505,
      "recall": 0.990990990990991,
      "f1": 0.7213114754098362,
      "num_samples": 200,
      "true_positives": 110,
      "true_negatives": 5,
      "false_positives": 84,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 0
    },
    "offpolicy_halueval": {
      "accuracy": 0.575,
      "precision": 0.9615384615384616,
      "recall": 0.22935779816513763,
      "f1": 0.37037037037037035,
      "num_samples": 200,
      "true_positives": 25,
      "true_negatives": 90,
      "false_positives": 1,
      "false_negatives": 84,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 0
    },
    "unanswerable": {
      "accuracy": 0.505,
      "precision": 1.0,
      "recall": 0.01,
      "f1": 0.019801980198019802,
      "num_samples": 200,
      "true_positives": 1,
      "true_negatives": 100,
      "false_positives": 0,
      "false_negatives": 99,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch0-1758146239-0c43a9eb-8f59bb71",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 0
    }
  },
  "epoch_1": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.555,
      "precision": 0.5526315789473685,
      "recall": 0.22580645161290322,
      "f1": 0.32061068702290074,
      "num_samples": 200,
      "true_positives": 21,
      "true_negatives": 90,
      "false_positives": 17,
      "false_negatives": 72,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 1
    },
    "offpolicy_doluschat": {
      "accuracy": 0.66,
      "precision": 0.6666666666666666,
      "recall": 0.6262626262626263,
      "f1": 0.6458333333333334,
      "num_samples": 200,
      "true_positives": 62,
      "true_negatives": 70,
      "false_positives": 31,
      "false_negatives": 37,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 1
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5822784810126582,
      "precision": 0.5882352941176471,
      "recall": 0.5128205128205128,
      "f1": 0.547945205479452,
      "num_samples": 79,
      "true_positives": 20,
      "true_negatives": 26,
      "false_positives": 14,
      "false_negatives": 19,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 1
    },
    "mask-factual": {
      "accuracy": 0.565,
      "precision": 0.6976744186046512,
      "recall": 0.28846153846153844,
      "f1": 0.4081632653061224,
      "num_samples": 200,
      "true_positives": 30,
      "true_negatives": 83,
      "false_positives": 13,
      "false_negatives": 74,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 1
    },
    "sycophancy": {
      "accuracy": 0.495,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 200,
      "true_positives": 0,
      "true_negatives": 99,
      "false_positives": 5,
      "false_negatives": 96,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 1
    },
    "sandbagging_ascii": {
      "accuracy": 0.725,
      "precision": 1.0,
      "recall": 0.46601941747572817,
      "f1": 0.6357615894039735,
      "num_samples": 200,
      "true_positives": 48,
      "true_negatives": 97,
      "false_positives": 0,
      "false_negatives": 55,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 1
    },
    "self-sycophancy": {
      "accuracy": 0.5051546391752577,
      "precision": 1.0,
      "recall": 0.04,
      "f1": 0.07692307692307693,
      "num_samples": 97,
      "true_positives": 2,
      "true_negatives": 47,
      "false_positives": 0,
      "false_negatives": 48,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 1
    },
    "sandbagging_other": {
      "accuracy": 0.51,
      "precision": 0.5,
      "recall": 0.10204081632653061,
      "f1": 0.1694915254237288,
      "num_samples": 200,
      "true_positives": 10,
      "true_negatives": 92,
      "false_positives": 10,
      "false_negatives": 88,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 1
    },
    "games": {
      "accuracy": 0.565,
      "precision": 0.5612244897959183,
      "recall": 0.990990990990991,
      "f1": 0.7166123778501629,
      "num_samples": 200,
      "true_positives": 110,
      "true_negatives": 3,
      "false_positives": 86,
      "false_negatives": 1,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 1
    },
    "offpolicy_halueval": {
      "accuracy": 0.555,
      "precision": 0.8571428571428571,
      "recall": 0.22018348623853212,
      "f1": 0.3503649635036496,
      "num_samples": 200,
      "true_positives": 24,
      "true_negatives": 87,
      "false_positives": 4,
      "false_negatives": 85,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 1
    },
    "unanswerable": {
      "accuracy": 0.51,
      "precision": 1.0,
      "recall": 0.02,
      "f1": 0.0392156862745098,
      "num_samples": 200,
      "true_positives": 2,
      "true_negatives": 100,
      "false_positives": 0,
      "false_negatives": 98,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch1-1758147935-7a1aa8c3-fdd32815",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 1
    }
  },
  "epoch_2": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.505,
      "precision": 0.42857142857142855,
      "recall": 0.1935483870967742,
      "f1": 0.26666666666666666,
      "num_samples": 200,
      "true_positives": 18,
      "true_negatives": 83,
      "false_positives": 24,
      "false_negatives": 75,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 2
    },
    "offpolicy_doluschat": {
      "accuracy": 0.67,
      "precision": 0.6666666666666666,
      "recall": 0.6666666666666666,
      "f1": 0.6666666666666666,
      "num_samples": 200,
      "true_positives": 66,
      "true_negatives": 68,
      "false_positives": 33,
      "false_negatives": 33,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 2
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5949367088607594,
      "precision": 0.64,
      "recall": 0.41025641025641024,
      "f1": 0.5,
      "num_samples": 79,
      "true_positives": 16,
      "true_negatives": 31,
      "false_positives": 9,
      "false_negatives": 23,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 2
    },
    "mask-factual": {
      "accuracy": 0.56,
      "precision": 0.6428571428571429,
      "recall": 0.34615384615384615,
      "f1": 0.45,
      "num_samples": 200,
      "true_positives": 36,
      "true_negatives": 76,
      "false_positives": 20,
      "false_negatives": 68,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 2
    },
    "sycophancy": {
      "accuracy": 0.45,
      "precision": 0.1111111111111111,
      "recall": 0.020833333333333332,
      "f1": 0.03508771929824561,
      "num_samples": 200,
      "true_positives": 2,
      "true_negatives": 88,
      "false_positives": 16,
      "false_negatives": 94,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 2
    },
    "sandbagging_ascii": {
      "accuracy": 0.79,
      "precision": 0.9841269841269841,
      "recall": 0.6019417475728155,
      "f1": 0.746987951807229,
      "num_samples": 200,
      "true_positives": 62,
      "true_negatives": 96,
      "false_positives": 1,
      "false_negatives": 41,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 2
    },
    "self-sycophancy": {
      "accuracy": 0.4742268041237113,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 97,
      "true_positives": 0,
      "true_negatives": 46,
      "false_positives": 1,
      "false_negatives": 50,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 2
    },
    "sandbagging_other": {
      "accuracy": 0.5,
      "precision": 0.47058823529411764,
      "recall": 0.16326530612244897,
      "f1": 0.24242424242424243,
      "num_samples": 200,
      "true_positives": 16,
      "true_negatives": 84,
      "false_positives": 18,
      "false_negatives": 82,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 2
    },
    "games": {
      "accuracy": 0.57,
      "precision": 0.5634517766497462,
      "recall": 1.0,
      "f1": 0.7207792207792209,
      "num_samples": 200,
      "true_positives": 111,
      "true_negatives": 3,
      "false_positives": 86,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 2
    },
    "offpolicy_halueval": {
      "accuracy": 0.6,
      "precision": 0.8918918918918919,
      "recall": 0.30275229357798167,
      "f1": 0.452054794520548,
      "num_samples": 200,
      "true_positives": 33,
      "true_negatives": 87,
      "false_positives": 4,
      "false_negatives": 76,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 2
    },
    "unanswerable": {
      "accuracy": 0.505,
      "precision": 1.0,
      "recall": 0.01,
      "f1": 0.019801980198019802,
      "num_samples": 200,
      "true_positives": 1,
      "true_negatives": 100,
      "false_positives": 0,
      "false_negatives": 99,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch2-1758149630-271aff7a-33f2d8a5",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 2
    }
  },
  "epoch_3": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.52,
      "precision": 0.4634146341463415,
      "recall": 0.20430107526881722,
      "f1": 0.2835820895522388,
      "num_samples": 200,
      "true_positives": 19,
      "true_negatives": 85,
      "false_positives": 22,
      "false_negatives": 74,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 3
    },
    "offpolicy_doluschat": {
      "accuracy": 0.665,
      "precision": 0.6568627450980392,
      "recall": 0.6767676767676768,
      "f1": 0.6666666666666667,
      "num_samples": 200,
      "true_positives": 67,
      "true_negatives": 66,
      "false_positives": 35,
      "false_negatives": 32,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 3
    },
    "cot-unfaithfulness": {
      "accuracy": 0.5443037974683544,
      "precision": 0.5517241379310345,
      "recall": 0.41025641025641024,
      "f1": 0.47058823529411764,
      "num_samples": 79,
      "true_positives": 16,
      "true_negatives": 27,
      "false_positives": 13,
      "false_negatives": 23,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 3
    },
    "mask-factual": {
      "accuracy": 0.58,
      "precision": 0.65625,
      "recall": 0.40384615384615385,
      "f1": 0.5000000000000001,
      "num_samples": 200,
      "true_positives": 42,
      "true_negatives": 74,
      "false_positives": 22,
      "false_negatives": 62,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 3
    },
    "sycophancy": {
      "accuracy": 0.475,
      "precision": 0.3333333333333333,
      "recall": 0.09375,
      "f1": 0.14634146341463417,
      "num_samples": 200,
      "true_positives": 9,
      "true_negatives": 86,
      "false_positives": 18,
      "false_negatives": 87,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 3
    },
    "sandbagging_ascii": {
      "accuracy": 0.895,
      "precision": 0.9767441860465116,
      "recall": 0.8155339805825242,
      "f1": 0.8888888888888887,
      "num_samples": 200,
      "true_positives": 84,
      "true_negatives": 95,
      "false_positives": 2,
      "false_negatives": 19,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 3
    },
    "self-sycophancy": {
      "accuracy": 0.4742268041237113,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 97,
      "true_positives": 0,
      "true_negatives": 46,
      "false_positives": 1,
      "false_negatives": 50,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 3
    },
    "sandbagging_other": {
      "accuracy": 0.52,
      "precision": 0.5208333333333334,
      "recall": 0.25510204081632654,
      "f1": 0.3424657534246575,
      "num_samples": 200,
      "true_positives": 25,
      "true_negatives": 79,
      "false_positives": 23,
      "false_negatives": 73,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 3
    },
    "games": {
      "accuracy": 0.57,
      "precision": 0.5634517766497462,
      "recall": 1.0,
      "f1": 0.7207792207792209,
      "num_samples": 200,
      "true_positives": 111,
      "true_negatives": 3,
      "false_positives": 86,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 3
    },
    "offpolicy_halueval": {
      "accuracy": 0.57,
      "precision": 0.8108108108108109,
      "recall": 0.27522935779816515,
      "f1": 0.410958904109589,
      "num_samples": 200,
      "true_positives": 30,
      "true_negatives": 84,
      "false_positives": 7,
      "false_negatives": 79,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 3
    },
    "unanswerable": {
      "accuracy": 0.5,
      "precision": 0.5,
      "recall": 0.02,
      "f1": 0.038461538461538464,
      "num_samples": 200,
      "true_positives": 2,
      "true_negatives": 98,
      "false_positives": 2,
      "false_negatives": 98,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch3-1758152388-d1645e95-2fb2240f",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 3
    }
  },
  "epoch_4": {
    "offpolicy_truthisuniversal": {
      "accuracy": 0.495,
      "precision": 0.4090909090909091,
      "recall": 0.1935483870967742,
      "f1": 0.26277372262773724,
      "num_samples": 200,
      "true_positives": 18,
      "true_negatives": 81,
      "false_positives": 26,
      "false_negatives": 75,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_truthisuniversal",
      "epoch": 4
    },
    "offpolicy_doluschat": {
      "accuracy": 0.655,
      "precision": 0.6442307692307693,
      "recall": 0.6767676767676768,
      "f1": 0.6600985221674877,
      "num_samples": 200,
      "true_positives": 67,
      "true_negatives": 64,
      "false_positives": 37,
      "false_negatives": 32,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_doluschat",
      "epoch": 4
    },
    "cot-unfaithfulness": {
      "accuracy": 0.569620253164557,
      "precision": 0.6086956521739131,
      "recall": 0.358974358974359,
      "f1": 0.45161290322580644,
      "num_samples": 79,
      "true_positives": 14,
      "true_negatives": 31,
      "false_positives": 9,
      "false_negatives": 25,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "cot-unfaithfulness",
      "epoch": 4
    },
    "mask-factual": {
      "accuracy": 0.595,
      "precision": 0.6885245901639344,
      "recall": 0.40384615384615385,
      "f1": 0.5090909090909091,
      "num_samples": 200,
      "true_positives": 42,
      "true_negatives": 77,
      "false_positives": 19,
      "false_negatives": 62,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "mask-factual",
      "epoch": 4
    },
    "sycophancy": {
      "accuracy": 0.47,
      "precision": 0.3333333333333333,
      "recall": 0.10416666666666667,
      "f1": 0.15873015873015875,
      "num_samples": 200,
      "true_positives": 10,
      "true_negatives": 84,
      "false_positives": 20,
      "false_negatives": 86,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sycophancy",
      "epoch": 4
    },
    "sandbagging_ascii": {
      "accuracy": 0.945,
      "precision": 0.96,
      "recall": 0.9320388349514563,
      "f1": 0.9458128078817734,
      "num_samples": 200,
      "true_positives": 96,
      "true_negatives": 93,
      "false_positives": 4,
      "false_negatives": 7,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_ascii",
      "epoch": 4
    },
    "self-sycophancy": {
      "accuracy": 0.4742268041237113,
      "precision": 0.0,
      "recall": 0.0,
      "f1": 0.0,
      "num_samples": 97,
      "true_positives": 0,
      "true_negatives": 46,
      "false_positives": 1,
      "false_negatives": 50,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "self-sycophancy",
      "epoch": 4
    },
    "sandbagging_other": {
      "accuracy": 0.54,
      "precision": 0.56,
      "recall": 0.2857142857142857,
      "f1": 0.37837837837837834,
      "num_samples": 200,
      "true_positives": 28,
      "true_negatives": 80,
      "false_positives": 22,
      "false_negatives": 70,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "sandbagging_other",
      "epoch": 4
    },
    "games": {
      "accuracy": 0.57,
      "precision": 0.5634517766497462,
      "recall": 1.0,
      "f1": 0.7207792207792209,
      "num_samples": 200,
      "true_positives": 111,
      "true_negatives": 3,
      "false_positives": 86,
      "false_negatives": 0,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "games",
      "epoch": 4
    },
    "offpolicy_halueval": {
      "accuracy": 0.555,
      "precision": 0.8333333333333334,
      "recall": 0.22935779816513763,
      "f1": 0.3597122302158274,
      "num_samples": 200,
      "true_positives": 25,
      "true_negatives": 86,
      "false_positives": 5,
      "false_negatives": 84,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "offpolicy_halueval",
      "epoch": 4
    },
    "unanswerable": {
      "accuracy": 0.5,
      "precision": 0.5,
      "recall": 0.01,
      "f1": 0.0196078431372549,
      "num_samples": 200,
      "true_positives": 1,
      "true_negatives": 99,
      "false_positives": 1,
      "false_negatives": 99,
      "model_ref": "fellows_safety/gpt-oss-120b-lie-mask-factual-epoch4-1758154284-759d1c37-85f69260",
      "is_endpoint": true,
      "is_baseline": false,
      "fold_name": "unanswerable",
      "epoch": 4
    }
  }
}