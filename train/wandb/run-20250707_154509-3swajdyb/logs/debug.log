2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_setup.py:_flush():81] Current SDK version is 0.20.1
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_setup.py:_flush():81] Configure stats pid to 4151997
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_setup.py:_flush():81] Loading settings from /home/dipika/.config/wandb/settings
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_setup.py:_flush():81] Loading settings from /mnt/nvme3/dipika/lie-detector/train/wandb/settings
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_setup.py:_flush():81] Loading settings from environment variables
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_init.py:setup_run_log_directory():703] Logging user logs to /mnt/nvme3/dipika/lie-detector/train/wandb/run-20250707_154509-3swajdyb/logs/debug.log
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_init.py:setup_run_log_directory():704] Logging internal logs to /mnt/nvme3/dipika/lie-detector/train/wandb/run-20250707_154509-3swajdyb/logs/debug-internal.log
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_init.py:init():831] calling init triggers
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_init.py:init():836] wandb.init called with sweep_config: {}
config: {'model': {'model_name': 'unsloth/Meta-Llama-3.1-70B', 'use_4bit': True, 'use_nested_quant': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True, 'bnb_4bit_quant_type': 'nf4', 'max_length': 2048, 'trust_remote_code': True, 'use_peft': True, 'lora_r': 16, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': 'q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj'}, 'data': {'train_file': '../lie_detection_training.jsonl', 'validation_split': 0.1, 'max_samples': None, 'prompt_template': '### Human: {prompt}\n\n### Assistant: {completion}'}, 'training': {'output_dir': '../llama-70b-alignment-faking-sft', 'num_train_epochs': 5, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'learning_rate': 2e-05, 'weight_decay': 0.01, 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 500, 'eval_steps': 500, 'eval_strategy': 'steps', 'save_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss', 'greater_is_better': False, 'fp16': True, 'dataloader_pin_memory': False, 'remove_unused_columns': False, 'report_to': 'wandb', 'run_name': 'llama-70b-alignment-faking-sft', 'early_stopping_patience': 3, 'early_stopping_threshold': 0.001}, 'seed': 42, 'use_wandb': True, '_wandb': {}}
2025-07-07 15:45:09,302 INFO    MainThread:4151997 [wandb_init.py:init():872] starting backend
2025-07-07 15:45:09,508 INFO    MainThread:4151997 [wandb_init.py:init():875] sending inform_init request
2025-07-07 15:45:09,511 INFO    MainThread:4151997 [wandb_init.py:init():883] backend started and connected
2025-07-07 15:45:09,512 INFO    MainThread:4151997 [wandb_init.py:init():956] updated telemetry
2025-07-07 15:45:09,516 INFO    MainThread:4151997 [wandb_init.py:init():980] communicating run to backend with 90.0 second timeout
2025-07-07 15:45:14,734 INFO    MainThread:4151997 [wandb_init.py:init():1032] starting run threads in backend
2025-07-07 15:45:14,902 INFO    MainThread:4151997 [wandb_run.py:_console_start():2453] atexit reg
2025-07-07 15:45:14,902 INFO    MainThread:4151997 [wandb_run.py:_redirect():2301] redirect: wrap_raw
2025-07-07 15:45:14,902 INFO    MainThread:4151997 [wandb_run.py:_redirect():2370] Wrapping output streams.
2025-07-07 15:45:14,903 INFO    MainThread:4151997 [wandb_run.py:_redirect():2393] Redirects installed.
2025-07-07 15:45:14,904 INFO    MainThread:4151997 [wandb_init.py:init():1078] run started, returning control to user process
